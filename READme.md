```markdown
# Multi‚ÄëProject Repository: NLP Pipelines & RAG‚ÄëPowered Quote Retrieval

This repository contains two end‚Äëto‚Äëend NLP projects:

1. **Customer Support Ticket Classification & Entity Extraction**  
2. **Semantic Quote Retrieval System with Retrieval‚ÄëAugmented Generation (RAG)**  

Each project is self‚Äëcontained with its own notebook/scripts, requirements, and instructions.

---

## üìÅ Repository Structure

```

/

‚îú‚îÄ‚îÄ customer_support_pipeline/

‚îÇ   ‚îú‚îÄ‚îÄ customer_support_ticket_pipeline.ipynb

‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt

‚îÇ   ‚îú‚îÄ‚îÄ ai_dev_assignment_tickets_complex_1000.xlsx

‚îÇ   ‚îî‚îÄ‚îÄ README.md

‚îÇ

‚îî‚îÄ‚îÄ quote_rag_system/

‚îú‚îÄ‚îÄ data_prep_finetune.ipynb

‚îú‚îÄ‚îÄ rag_pipeline_evaluate.ipynb

‚îú‚îÄ‚îÄ streamlit_app.py

‚îú‚îÄ‚îÄ requirements.txt

‚îî‚îÄ‚îÄ README.md

```

---

## 1. Customer Support Ticket Classification & Entity Extraction

### üìñ Project Overview
Builds a classical‚ÄëML pipeline that:

- **Classifies** each ticket by **issue type** (multi‚Äëclass)  
- **Classifies** ticket **urgency** (Low/Medium/High)  
- **Extracts** entities: product names, dates, complaint keywords  
- Exposes an **integration** function for real‚Äëtime inference  
- (Optional) Launches a **Gradio** interface for interactive use  

### üóí Key Components

1. **Data Preparation**  
   - Load Excel file `ai_dev_assignment_tickets_complex_1000.xlsx`  
   - Clean text: lowercase, remove punctuation/numbers ‚Å†  
   - Tokenize + remove stopwords + lemmatize  
   - Handle missing values  

2. **Feature Engineering**  
   - **TF‚ÄëIDF** vectorization  
   - Ticket‚Äëlevel features: text length, sentiment score  

3. **Modeling**  
   - **Random Forest** for issue‚Äëtype classification  
   - **Logistic Regression** for urgency classification  
   - Train/test split + evaluation (classification report, confusion matrix)  

4. **Entity Extraction**  
   - Rule‚Äëbased lookup from a product list  
   - Regex for dates (e.g., ‚Äú15/05/2025‚Äù, ‚Äú15th May‚Äù)  
   - Keyword matching for complaints (e.g., ‚Äúbroken‚Äù, ‚Äúerror‚Äù)  

5. **Integration Function**  
   ```python
   def analyze_ticket(text: str) -> dict:
       """
       Returns:
         - predicted_issue_type: str
         - predicted_urgency_level: str
         - extracted_entities: dict
       """
```

6. **Gradio Interface (Optional)**
   * Interactive web UI: input raw text, view predictions

### ‚öôÔ∏è Setup & Usage

1. **Navigate** into the folder:
   ```bash
   cd customer_support_pipeline
   ```
2. **Create** a virtual environment & install dependencies:
   ```bash
   python3 -m venv venv
   source venv/bin/activate
   pip install -r requirements.txt
   ```
3. **Launch Notebook** for development & evaluation:
   ```bash
   jupyter notebook customer_support_ticket_pipeline.ipynb
   ```
4. **Run Gradio App** (if desired):
   ```bash
   python customer_support_ticket_pipeline.ipynb  # within a notebook cell
   ```
5. **Integration** in other scripts:
   ```python
   from pipeline import analyze_ticket
   result = analyze_ticket("My device stopped working on 12th June.")
   ```

---

## 2. Semantic Quote Retrieval System with RAG

### üìñ Project Overview

Implements a Retrieval‚ÄëAugmented Generation system that:

* **Fine‚Äëtunes** a SentenceTransformer on the Abirate/english_quotes dataset
* **Indexes** all quotes with FAISS for efficient similarity search
* **Augments** an LLM (e.g., GPT‚Äë3.5/4) to answer natural‚Äëlanguage quote queries
* **Evaluates** retrieval quality via sample queries & (optionally) RAGAS/Quotient
* **Deploys** a **Streamlit** web app for end‚Äëuser interaction

### üóí Key Components

1. **Data Preparation**

   ```python
   from datasets import load_dataset
   ds = load_dataset("Abirate/english_quotes")
   df = pd.DataFrame(ds["train"])
   df["quote_clean"] = df.quote.str.strip().dropna()
   ```
2. **Model Fine‚ÄëTuning**

   * Base model: `all-MiniLM-L6-v2`
   * Loss: `MultipleNegativesRankingLoss` on (quote, author+tags) pairs
   * Output: `fine_tuned_quote_sbert/`
3. **Indexing with FAISS**

   ```python
   embeddings = model.encode(df.quote_clean.tolist())
   index = faiss.IndexFlatL2(embeddings.shape[1])
   index.add(embeddings)
   ```
4. **RAG Pipeline**

   * **Retriever** : FAISS-wrapped LangChain retriever
   * **Generator** : OpenAI LLM via `langchain.chains.RetrievalQA`
   * **Prompt** : instruct LLM to output JSON {quotes, authors, tags, similarity_scores}
5. **Evaluation**

   * Sample queries (‚ÄúEinstein insanity quotes‚Äù, ‚Äúaccomplishment‚Äù)
   * Print JSON responses + source documents
   * (Optional) integrate with RAGAS, Quotient, or Arize for quantitative metrics
6. **Streamlit App** (`streamlit_app.py`)

   ```bash
   streamlit run streamlit_app.py
   ```

   * Query input box
   * JSON response display
   * Top‚Äëk quotes with score captions

### ‚öôÔ∏è Setup & Usage

1. **Navigate** into the folder:
   ```bash
   cd quote_rag_system
   ```
2. **Create** virtual environment & install:
   ```bash
   python3 -m venv venv
   source venv/bin/activate
   pip install -r requirements.txt
   ```
3. **Data Prep & Fine‚ÄëTune**
   ```bash
   jupyter notebook data_prep_finetune.ipynb
   ```
4. **Build & Evaluate RAG**
   ```bash
   jupyter notebook rag_pipeline_evaluate.ipynb
   ```
5. **Launch Streamlit App**
   ```bash
   streamlit run streamlit_app.py
   ```

---

## üé• Demo Video

A short screencast demonstrating:

* Data loading & cleaning
* Model training & indexing
* Example RAG queries & evaluation
* Interactive Streamlit application

Please refer to [DEMO_LINK_PLACEHOLDER] for the video walkthrough.

---

## üìù Notes & Best Practices

* **Reproducibility:** use fixed random seeds for dataset splits & model training.
* **Performance:** adjust `max_features` for TF‚ÄëIDF or `k` in retrieval to trade off accuracy vs. speed.
* **Extensibility:** swap in more powerful LLMs (e.g., GPT‚Äë4, Llama 3) or vector stores (Chroma, Pinecone).
* **Security:** do not commit API keys; load them via environment variables (`OPENAI_API_KEY`).

---

Thank you for exploring these pipelines!

For questions or issues, please open a GitHub issue or contact the maintainer.
